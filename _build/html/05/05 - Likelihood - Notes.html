

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>5 Likelihood &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '05/05 - Likelihood - Notes';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">BASIC PROBABILITY</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../02/02solutions.html">Probability 1: Discrete distributions</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">STATISTICAL INFERENCE</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../06/markdown.html">06 Maximul Likelihood Estimator</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../06/practical.html">6.1 R workshop for Maximum Likelihood estimation</a></li>

<li class="toctree-l2"><a class="reference internal" href="../06/solutions.html">06 Maximum Likelihood Estimation Solutions</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F05/05 - Likelihood - Notes.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/05/05 - Likelihood - Notes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>5 Likelihood</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-versus-inference">5.1 Probability versus Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">5.2 Maximum likelihood estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood">5.3 The likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-likelihoods">5.4 Examples of likelihoods¶</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-model">5.4.1 Binomial model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-model">5.4.2 Exponential model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">5.5 Log likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-mle">5.6 Finding the MLE</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">5.6.1 Binomial model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5.6.2 Exponential model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.7 Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="likelihood">
<h1>5 Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this heading">#</a></h1>
<div class="alert alert-block alert-warning">
<b> Intended learning objectives:</b> 
<p>By the end of this session you should be able to:</p>
<ul class="simple">
<li><p>explain the concepts of likelihood and maximum likelihood estimation</p></li>
<li><p>derive a likelihood in a simple situation</p></li>
<li><p>explain the connection between maximising the likelihood and maximising the log-likelihood</p></li>
<li><p>describe and apply the process of obtaining a maximum likelihood estimator</p></li>
</ul>
</div><section id="probability-versus-inference">
<h2>5.1 Probability versus Inference<a class="headerlink" href="#probability-versus-inference" title="Permalink to this heading">#</a></h2>
<p>A typical probability problem is as follows. We are planning to run a small clinical study, which involves giving 8 patients a particular drug. We are told that the probability that a single patient experiences a side effect from a particular drug is 0.23. From this information, we can calculate the probability of various complex events occurring. For example, we might want to know the probability that more than 6 of the 8 patients will experience a side effect. Or we might wish to know the probability that none of the 8 patients experience a side effect. Here, we are assuming that a characteristic (parameter) of the population is known. Specifically, we are assuming that we know the true probability of a single patient experiencing a side effect.</p>
<p>Real life is not like that! Typically, in health data science studies, we have observed some data which we believe can be modelled using a particular distribution, but the parameters of that distribution are unknown. For the small clinical study, for example, in real life we would observe how many of the 8 patients did in fact experience a side effect. We might be happy to assume that these data are drawn from a binomial distribution. But the probability of a patient experiencing a side effect - the key parameter of the binomial distribution - would be unknown. The study aim would be to make statements - inferences - about that unknown parameter. So, in some sense, the problem is the opposite way around.</p>
<p>Our task, in statistical inference, is to make statements about the underlying parameter(s) of our proposed model given the observed data. In particular, we typically wish to obtain the best estimate of the unknown parameters. In the simple clinical study, we would want to obtain the best estimate of the unknown probability of experiencing a side effect, given the observed information about how many of the 8 patients did experience a side effect. We also wish to know how well we have estimated the unknown parameter(s): what is the uncertainty associated with our estimate(s)?</p>
<p>The concept of likelihood provides the best single framework for this task. We will see that the likelihood function, often simply called the likelihood, plays a fundamental role in both frequentist and Bayesian inference.</p>
</section>
<section id="maximum-likelihood-estimation">
<h2>5.2 Maximum likelihood estimation<a class="headerlink" href="#maximum-likelihood-estimation" title="Permalink to this heading">#</a></h2>
<p>Consider the example above, of a small clinical study of 8 patients who are all given a particular drug. The observed data consist of the number, of those 8 patients, who experience a side effect. Suppose that we conduct the study and observe that 2 patients experience a side effect. We wish to use these observed data to make statements - inferences - about the unknown probability of experiencing a side effect from that drug.</p>
<p><strong>Statistical model:</strong> We begin by defining a model for the data. Here, we define <span class="math notranslate nohighlight">\(X\)</span> as the random variable representing the total number of the 8 patients who experience a side effect. Our model is that</p>
<div class="math notranslate nohighlight">
\[
X \sim binomial(8, \pi)
\]</div>
<p>which - we remember from the probability sessions - involves the assumptions that each Bernoulli event (whether or not each individual patient experiences a side effect) is independent and has the same probability of occurring.</p>
<p>This model involves the unknown parameter <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>Data:</strong> We have observed a realisation from this model, <span class="math notranslate nohighlight">\(X=2\)</span>. These are often called our observed data.</p>
<p>Under our proposed statistical model, the probability that 2 out of 8 patients experience a side effect is:</p>
<div class="math notranslate nohighlight">
\[
P(X=2) = {8 \choose 2} \pi^2 (1-\pi)^6
\]</div>
<p>Since <span class="math notranslate nohighlight">\(\pi\)</span> is unknown, it is natural to consider how the probability of observing these data varies with different values of <span class="math notranslate nohighlight">\(\pi\)</span>:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><span class="math notranslate nohighlight">\(\pi\)</span></p></th>
<th class="head text-center"><p>P(<span class="math notranslate nohighlight">\(X\)</span>=<span class="math notranslate nohighlight">\(2\)</span>)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>0</p></td>
<td class="text-center"><p>0</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>0.25</p></td>
<td class="text-center"><p>0.311</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>0.5</p></td>
<td class="text-center"><p>0.109</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>0.75</p></td>
<td class="text-center"><p>0.004</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>1</p></td>
<td class="text-center"><p>0</p></td>
</tr>
</tbody>
</table>
<p>Suppose that, in truth, the unknown probability of a patient experiencing a side effect from this drug was 0.75. The probability of then observing 2 from 8 patients experiencing a side effect is 0.004. This is a very low probability, so this would be an unusual or perhaps unexpected event, although not strictly impossible.</p>
<p>Suppose that, conversely, the unknown probability of a patient experiencing a side effect from this drug was actually 0.25. Then the probability of observing 2 from 8 patients experiencing a side effect would be 0.31 (<span class="math notranslate nohighlight">\(31\%\)</span>). If this were the case, there would be nothing unusual or unexpected about our observed data.</p>
<p>We do not know which value of <span class="math notranslate nohighlight">\(\pi\)</span> is the true value. But a sensible strategy to obtain a ‘best guess’, or estimate, of <span class="math notranslate nohighlight">\(\pi\)</span>, might be to pick the value which maximises the probability of observing the data that we observed. We will see below that this probability is in fact the likelihood, leading to the concept of maximising the likelihood or maximum likelihood. This is a term that you will encounter frequently in statistics.</p>
<p>Following these ideas, we can extend the table above by considering a finer range of possible values for <span class="math notranslate nohighlight">\(\pi\)</span> between 0 and 1, and plot the probability of observing <span class="math notranslate nohighlight">\(X=2\)</span>, assuming that that value of <span class="math notranslate nohighlight">\(\pi\)</span> were true. This gives the graph below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a range of values for pi</span>
<span class="kc">pi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">)</span>

<span class="c1"># Calculate the likelihood for each value, given n=8 and x=2</span>
<span class="n">L_pi</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">choose</span><span class="p">(</span><span class="m">8</span><span class="p">,</span><span class="m">2</span><span class="p">)</span><span class="o">*</span><span class="kc">pi</span><span class="o">^</span><span class="m">2</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="kc">pi</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">8-2</span><span class="p">)</span>

<span class="c1"># Plot the output</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">pi</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L_pi</span><span class="p">)</span>

<span class="c1"># Add a line to indicate the value which yields the highest likelihood</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_pi</span><span class="p">)],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/9d15503ab3d8417f56ebdb8d2cc8991cc43ac320061ecb89fa623a1c47414cd1.png" src="../_images/9d15503ab3d8417f56ebdb8d2cc8991cc43ac320061ecb89fa623a1c47414cd1.png" />
</div>
</div>
<p>We see that <span class="math notranslate nohighlight">\(\pi=0.25\)</span> is the value that leads to the highest probability of observing the data that we did indeed observe (i.e <span class="math notranslate nohighlight">\(X=2\)</span>) so we choose this value as our best guess for <span class="math notranslate nohighlight">\(\pi\)</span>. We will see that this value is called the maximum likelihood estimator. We write <span class="math notranslate nohighlight">\(\hat{\pi} = 0.25\)</span>, where we have added a hat to indicate that this is being viewed as an estimate of an unknown parameter.</p>
<p>The likelihood when <span class="math notranslate nohighlight">\(\pi = 0\)</span> is exactly zero, as is the likelihood when <span class="math notranslate nohighlight">\(\pi = 1\)</span>. This makes sense because these two probabilities would make the observed data impossible - they imply that patients would either <em>never</em> or <em>always</em> experience side effects. Informally, we could say that these values are <em>inconsistent</em> with the data.</p>
<p>Note that, our estimate of the probability of a patient experiencing a side effect is intuitively a sensible one: it is the sample proportion, <span class="math notranslate nohighlight">\(\frac{2}{8}\)</span>.</p>
<p>We will see later on that estimators obtained in this way (by maximising a likelihood) have very nice statistical properties.</p>
</section>
<section id="the-likelihood">
<h2>5.3 The likelihood<a class="headerlink" href="#the-likelihood" title="Permalink to this heading">#</a></h2>
<p>The function that we maximised above to find our estimate for the unknown parameter <span class="math notranslate nohighlight">\(\pi\)</span> took the same algebraic appearance as the probability distribution function, evaluated at the value of the observed data. We will see below that this function is called the likelihood. The likelihood looks like a probability distribution function. It has a probabilistic interpretation for any particular value of <span class="math notranslate nohighlight">\(\pi\)</span>: it’s the probability of seeing the observed data assuming that is the true value of <span class="math notranslate nohighlight">\(\pi\)</span>. However, in contrast to the probability distribution function, which is a function of <span class="math notranslate nohighlight">\(x\)</span> and sums to 1 over all possible values of <span class="math notranslate nohighlight">\(x\)</span>, the likelihood function is a function of <span class="math notranslate nohighlight">\(\pi\)</span>. So, for example, this does not sum to 1 over all possible values of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>A general definition of the likelihood is as follows.</p>
<p>For a probability model with parameter <span class="math notranslate nohighlight">\(\theta\)</span>, the likelihood of the parameter <span class="math notranslate nohighlight">\(\theta\)</span> given the observed data <span class="math notranslate nohighlight">\(x\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
L(\theta | x) = P(x | \theta)
\]</div>
<p>On the right hand side of this equation:</p>
<ul class="simple">
<li><p>This is either a probability distribution function or a density function</p></li>
<li><p>If our distribution is discrete, as above, this is: <span class="math notranslate nohighlight">\(P(x | \theta) = P(X=x)\)</span></p></li>
<li><p>If our distribution is continuous, this becomes: <span class="math notranslate nohighlight">\(P(x | \theta) = f(x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(P(x | \theta)\)</span> is a probability statement. It is the probability of seeing the observed data, under the assumed model, assuming that the true parameter value is equal to <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ul>
<p>And on the left hand side of this equation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(L(\theta | x)\)</span> is the likelihood function, often just called the likelihood.</p></li>
</ul>
<p>In an informal sense the likelihood conveys the <em>consistency</em> of different values of the parameter with the observed data.</p>
<p>We often just write the likelihood as <span class="math notranslate nohighlight">\(L(\theta)\)</span>. The additional notation (writing “<span class="math notranslate nohighlight">\(| x\)</span>”) is merely to remind ourselves that the likelihood function involves the observed data, but it is not a function of these: <span class="math notranslate nohighlight">\(x\)</span> is treated as a fixed quantity in the likelihood.</p>
</section>
<section id="examples-of-likelihoods">
<h2>5.4 Examples of likelihoods¶<a class="headerlink" href="#examples-of-likelihoods" title="Permalink to this heading">#</a></h2>
<section id="binomial-model">
<h3>5.4.1 Binomial model<a class="headerlink" href="#binomial-model" title="Permalink to this heading">#</a></h3>
<p>Consider a diabetes clinic at which patients present following initial diagnosis. The first line of intervention for diabetes is lifestyle change, and the clinician wants to determine what proportion of patients will respond to this intervention. She decides to conduct a study by following up twenty patients who present to the clinic in one day.</p>
<p><strong>Statistical model:</strong> We assume that a binomial model is appropriate for the number of patients who will respond to lifestyle changes out of the twenty patients in total.</p>
<div class="math notranslate nohighlight">
\[ X \sim Bin(20, \pi) \]</div>
<p><strong>Data:</strong> Out of the twenty patients sampled, she found that twelve of them had responded well after six weeks of recommended lifestyle changes. Our observed data are <span class="math notranslate nohighlight">\(x = 12\)</span>.</p>
<p><strong>Probability mass function:</strong> As we described before, the likelihood of <span class="math notranslate nohighlight">\(\pi\)</span> given these data is the probability of observing the data for different values for <span class="math notranslate nohighlight">\(\pi\)</span>. Remember the PMF for a binomial distribution of size 20,</p>
<div class="math notranslate nohighlight">
\[ P(X = x|\pi) = {20 \choose x} \pi^{x} (1-\pi)^{20 - x} \]</div>
<p>for a given value of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p><strong>Likelihood:</strong> The likelihood has this same form but is viewed as a function of <span class="math notranslate nohighlight">\(\pi\)</span>, rather than a function of <span class="math notranslate nohighlight">\(x\)</span>. For our observed data of 12 out of 20 patients,</p>
<div class="math notranslate nohighlight">
\[\begin{split} L(\pi | x = 12) = {20 \choose 12} \pi^{20} (1-\pi)^{20 - 12} \\  \end{split}\]</div>
<p>As before, we can identify the value of <span class="math notranslate nohighlight">\(\pi\)</span> which gives the maximum likelihood by plotting the likelihood for a range of values of <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>

<span class="c1"># Define a range of values for pi</span>
<span class="kc">pi</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">)</span>

<span class="c1"># Calculate the likelihood for each value, this time given n=20 and x=12</span>
<span class="n">L_pi</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">choose</span><span class="p">(</span><span class="m">20</span><span class="p">,</span><span class="m">12</span><span class="p">)</span><span class="o">*</span><span class="kc">pi</span><span class="o">^</span><span class="m">12</span><span class="o">*</span><span class="p">(</span><span class="m">1</span><span class="o">-</span><span class="kc">pi</span><span class="p">)</span><span class="o">^</span><span class="p">(</span><span class="m">20-12</span><span class="p">)</span>

<span class="c1"># Plot the output</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">pi</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L_pi</span><span class="p">)</span>

<span class="c1"># Find the value of pi for which L_pi is highest</span>
<span class="n">pi_max</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_pi</span><span class="p">)]</span>

<span class="c1"># Add a line to the plot at pi_max</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pi_max</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>

<span class="c1"># Add a title specifying the value of pi_max</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">pi_max</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b5ebb17bcfc8e0089ff7aa3638b8927c81cf1e50dd6285a0993ab00ae6ae9894.png" src="../_images/b5ebb17bcfc8e0089ff7aa3638b8927c81cf1e50dd6285a0993ab00ae6ae9894.png" />
</div>
</div>
<p>The value which maximises this function is 0.6, the observed sample proportion; we’ll call this value <span class="math notranslate nohighlight">\(\hat{\pi}\)</span> to indicate that it is an estimate of <span class="math notranslate nohighlight">\(\pi\)</span>. Notice that the likelihood for values of <span class="math notranslate nohighlight">\(\pi\)</span> smaller than 0.3 or greater than 0.9 is very small - much smaller than that of values around 0.6 - suggesting that these values are inconsistent with the observed data.</p>
</section>
<section id="exponential-model">
<h3>5.4.2 Exponential model<a class="headerlink" href="#exponential-model" title="Permalink to this heading">#</a></h3>
<p>Suppose we wish to estimate how long patients usually wait in reception before their GP appointment. At one practice, a patient walks through the door and the receptionist records the time until they get called through.</p>
<p><strong>Statistical model:</strong> The waiting time in minutes, <span class="math notranslate nohighlight">\(Y\)</span>, is a continuous random variable which must be non-negative. It is common to use an exponential distribution to model waiting times, so we will assume it’s a reasonable choice for this example.</p>
<div class="math notranslate nohighlight">
\[ Y \sim Exp(\lambda) \]</div>
<p><em>Remember that the mean of this distribution is equal to one over the rate parameter <span class="math notranslate nohighlight">\(\lambda\)</span>, i.e. <span class="math notranslate nohighlight">\(E(Y) = \frac{1}{\lambda}\)</span>.</em></p>
<p><strong>Data:</strong> The receptionist observes that the patient waits for eight minutes and forty-five seconds, so <span class="math notranslate nohighlight">\(y = 8.75\)</span>.</p>
<p><strong>Probability density function:</strong> The PDF for an exponential distribution is</p>
<div class="math notranslate nohighlight">
\[f_Y(y|\lambda) = \lambda e^{-y\lambda}\]</div>
<p><strong>Likelihood:</strong> We write down the likelihood for <span class="math notranslate nohighlight">\(\lambda\)</span> based on the exponential PDF above.</p>
<div class="math notranslate nohighlight">
\[ L(\lambda | y = 8.75) = \lambda e^{-8.75\lambda}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>

<span class="c1"># Define a range of values for lambda, equating to mean waiting times from 1 to 100 minutes</span>
<span class="n">lam</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">seq</span><span class="p">(</span><span class="m">0.01</span><span class="p">,</span><span class="m">1</span><span class="p">,</span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">)</span>

<span class="c1"># Calculate the likelihood for each value, given y=8.75</span>
<span class="n">L_lam</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lam</span><span class="o">*</span><span class="nf">exp</span><span class="p">(</span><span class="m">-8.75</span><span class="o">*</span><span class="n">lam</span><span class="p">)</span>

<span class="c1"># Find the value of lambda for which L_lam is highest</span>
<span class="n">lam_max</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lam</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">L_lam</span><span class="p">)]</span>

<span class="c1"># Plot the likelihood and indicate the maximum value</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lam</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L_lam</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lam_max</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">lam_max</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/aaab9ef3de5cd38cf9a0476f35dec707babc469f6cafc215e70d03565c7339eb.png" src="../_images/aaab9ef3de5cd38cf9a0476f35dec707babc469f6cafc215e70d03565c7339eb.png" />
</div>
</div>
<p>If we evaluate over a fine enough range of values for <span class="math notranslate nohighlight">\(\lambda\)</span>, we find that the value which maximises this exponential likelihood is equal to <span class="math notranslate nohighlight">\(\frac{1}{8.75}\)</span>, i.e. one over the observed waiting time. This defines an exponential distribution with mean equal to the observed waiting time.</p>
<p>As with the binomial example, the estimate obtained by maximising the likelihood is intuitively sensible based on the data we’ve observed.</p>
</section>
</section>
<section id="log-likelihood">
<h2>5.5 Log likelihood<a class="headerlink" href="#log-likelihood" title="Permalink to this heading">#</a></h2>
<p>We have discussed the idea that finding the maximum value of a likelihood gives us sensible estimates for the unknown parameters. For the examples above it is relatively clear from calculating a few values of the likelihood where the maximum lies, but this will not always be the case.</p>
<p>A theoretical result which will come in handy is that a value which maximises the likelihood also maximises the log-transform of the likelihood, or the <em>log-likelihood</em>. This is because the log is a <em>concave</em> function, so when we use it to transform the likelihood, any maximum or minimum stays in the same place on the x-axis. We will denote the log-likelihood <span class="math notranslate nohighlight">\(l(\theta) = \log(L(\theta))\)</span>.</p>
<p>This result is evident when plotting the transformation of the two likelihoods above.</p>
<p>For the binomial example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>

<span class="c1"># likelihood L(pi)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">pi</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L_pi</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pi_max</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">pi_max</span><span class="p">))</span>

<span class="c1"># log-likelihood l(pi)</span>
<span class="n">l_pi</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">L_pi</span><span class="p">)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">pi</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">l_pi</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_pi</span><span class="p">)],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span><span class="w"> </span><span class="kc">pi</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_pi</span><span class="p">)]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8ac3efdbd4e4802ae06cad73d310a8afd75fc893dbbab3c2b8df496689ade9fb.png" src="../_images/8ac3efdbd4e4802ae06cad73d310a8afd75fc893dbbab3c2b8df496689ade9fb.png" />
</div>
</div>
<p>For the exponential example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">options</span><span class="p">(</span><span class="n">repr.plot.width</span><span class="o">=</span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="n">repr.plot.height</span><span class="o">=</span><span class="m">6</span><span class="p">)</span>
<span class="nf">par</span><span class="p">(</span><span class="n">mfrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">2</span><span class="p">))</span>

<span class="c1"># likelihood L(beta)</span>
<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lam</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">L_lam</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lam_max</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">lam_max</span><span class="p">,</span><span class="m">2</span><span class="p">)))</span>

<span class="c1"># log-likelihood l(beta)</span>
<span class="n">l_lam</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">L_lam</span><span class="p">)</span>

<span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lam</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">l_lam</span><span class="p">)</span>
<span class="nf">abline</span><span class="p">(</span><span class="n">v</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lam</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_lam</span><span class="p">)],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
<span class="nf">title</span><span class="p">(</span><span class="nf">paste</span><span class="p">(</span><span class="s">&quot;Maximum at&quot;</span><span class="p">,</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">lam</span><span class="p">[</span><span class="nf">which.max</span><span class="p">(</span><span class="n">l_lam</span><span class="p">)],</span><span class="m">2</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2d0b10d2e1e265724ef462c23656c724841eea281b8a11a12890d8594b200778.png" src="../_images/2d0b10d2e1e265724ef462c23656c724841eea281b8a11a12890d8594b200778.png" />
</div>
</div>
<p>Log-transformed likelihoods are generally “better-behaved” and easier to work with than the original form. Remember the rules of logs for products and powers - we’ll see in the next section how these make computation with the log-likelihood very convenient.</p>
</section>
<section id="finding-the-mle">
<h2>5.6 Finding the MLE<a class="headerlink" href="#finding-the-mle" title="Permalink to this heading">#</a></h2>
<p>We’ve so far obtained the maximum likelihood estimate (MLE) by plotting the likelihood for different parameter values and looking for that which yields the maximum. Of course, the estimate obtained in this way depends on how many parameter values we evaluate.</p>
<p>The more formal way is to determine the location of that maximal point algebraically, from the likelihood function itself. In this way, we can directly obtain the general form for the MLE in terms of the data.</p>
<p>This is where the log-likelihood comes into its own; we know that a value which maximises the log-likelihood also maximises the likelihood, and the impact of logs on products and powers make the algebra much simpler.</p>
<p>We find the maximum likelihood estimator of a parameter from the log-likelihood function through the following steps:</p>
<div class="alert alert-success">
<p><b> Method for finding MLEs:</b></p>
<ol class="arabic simple">
<li><p>Obtain the derivative of the log-likelihood: <span class="math notranslate nohighlight">\(\frac{d l(\theta \mid {x})}{d \theta}\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\frac{d l(\theta \mid {x})}{d \theta}=0\)</span> and solve for <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>Verify that it is a maximum by showing that the second derivative <span class="math notranslate nohighlight">\(\frac{d ^2 l(\theta \mid  {x})}{d \theta ^2 }\)</span> is negative when the MLE is substituted for <span class="math notranslate nohighlight">\(\theta\)</span>.</p></li>
</ol>
</div>
<section id="id1">
<h3>5.6.1 Binomial model<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>We will derive the MLE for the binomial example described earlier. In general, the likelihood given observed data of <span class="math notranslate nohighlight">\(x\)</span> responders out of <span class="math notranslate nohighlight">\(n\)</span> patients is</p>
<div class="math notranslate nohighlight">
\[\begin{split} L(\pi|x) = {n \choose x} \pi^{x} (1-\pi)^{n - x} \\  \end{split}\]</div>
<p>and so the log-likelihood is</p>
<div class="math notranslate nohighlight">
\[\begin{split} l(\pi|x) = \log\left({n \choose x} \pi^{x} (1-\pi)^{n - x}\right) \\ = \log {n \choose x} + x \log \pi + (n-x)\log (1-\pi) \end{split}\]</div>
<p>We can now obtain the maximum likelihood estimate from this function.</p>
<p><strong>Step 1</strong>: Differentiate the log-likelihood with respect to our parameter <span class="math notranslate nohighlight">\(\pi\)</span></p>
<p>\begin{equation}
\frac{d l \left( \pi \mid x \right)}{d \pi} =  \frac{x}{\pi}  -\frac{(n-x)}{(1-\pi)}
\end{equation}</p>
<p><strong>Step 2</strong>: We set the derivative equal to zero and solve for <span class="math notranslate nohighlight">\(\pi\)</span></p>
<p>\begin{equation}
0 =  \frac{x}{\pi}  -\frac{(n-x)}{(1-\pi)}
\end{equation}</p>
<p>\begin{equation}
\frac{(n-x)}{(1-\pi)} =  \frac{x}{\pi}<br />
\end{equation}</p>
<p>\begin{equation}
(n-x)\pi =  x(1-\pi)
\end{equation}</p>
<p>\begin{equation}
n\pi-x\pi =  x-x\pi
\end{equation}</p>
<p>\begin{equation}
\hat{\pi} =  \frac{x}{n}
\end{equation}</p>
<p>Having solved the equation, we get that the maximum likelihood estimator for <span class="math notranslate nohighlight">\(\pi\)</span> is <span class="math notranslate nohighlight">\(\hat{\pi} =  \frac{x}{n}\)</span> (note that we put a hat to indicate that it is an estimator).</p>
<p>There is one thing left for us to check: we have found that <span class="math notranslate nohighlight">\(\frac{x}{n}\)</span> is the point where the derivative of the log-likelihood is zero, but that could mean that it is a maximum or a minimum of the log-likelihood function. To verify that this is indeed a maximum, we need to compute the second derivative of the log-likelihood and check that it takes a negative value when <span class="math notranslate nohighlight">\({\pi} =  \frac{x}{n}\)</span>.</p>
<p><strong>Step 3</strong>: Find the second derivative:</p>
<p>\begin{equation}
\frac{d l^2 \left( \pi \mid x \right)}{d \pi ^2} =  -\frac{x}{\pi^2}  -\frac{(n-x)}{(1-\pi)^2}
\end{equation}</p>
<p>This second derivative must be negative if we plug in <span class="math notranslate nohighlight">\(\frac{x}{n}\)</span> for <span class="math notranslate nohighlight">\({\pi}\)</span>. Both fractions on the right hand side have a squared number in the denominator which is positive, so we only have to think about the numerators. We have that <span class="math notranslate nohighlight">\(-x \leq 0 \)</span> since <span class="math notranslate nohighlight">\(x \geq 0\)</span> and also <span class="math notranslate nohighlight">\(-(n-x) \leq 0 \)</span> since <span class="math notranslate nohighlight">\(n \geq x \geq 0\)</span>. Thus the MLE for <span class="math notranslate nohighlight">\(\pi\)</span> is <span class="math notranslate nohighlight">\(\frac{x}{n}\)</span>.</p>
</section>
<section id="id2">
<h3>5.6.2 Exponential model<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>The likelihood function for the exponential example is</p>
<div class="math notranslate nohighlight">
\[L(\lambda | y) = \lambda e^{-\lambda y}\]</div>
<p>therefore we have</p>
<div class="math notranslate nohighlight">
\[ l(\lambda | y) = \log \left( \lambda e^{-\lambda y} \right) = \log \lambda - y \lambda \]</div>
<p><strong>Step 1</strong>: Differentiate the log-likelihood with respect to our parameter <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<div class="math notranslate nohighlight">
\[\frac{d l \left( \lambda \mid y \right)}{d \lambda} = \frac{1}{\lambda} - y\]</div>
<p><strong>Step 2</strong>: Set the derivative to zero and solve for <span class="math notranslate nohighlight">\(\lambda\)</span></p>
<div class="math notranslate nohighlight">
\[ \frac{1}{\lambda} - y = 0\]</div>
<div class="math notranslate nohighlight">
\[ \hat{\lambda} = \frac{1}{y} \]</div>
<p><strong>Step 3</strong>: Verify that this is a maximum rather than a minimum by considering the second derivative</p>
<div class="math notranslate nohighlight">
\[ \frac{d l^2 \left(\lambda \mid y \right)}{d \lambda ^2} = -\frac{1}{\lambda^2}\]</div>
<p>This is negative for any value of <span class="math notranslate nohighlight">\(\lambda\)</span>. So the MLE for <span class="math notranslate nohighlight">\(\lambda\)</span> is <span class="math notranslate nohighlight">\(\hat{\lambda} = \frac{1}{y}\)</span>, one over the observed waiting time.</p>
</section>
</section>
<section id="summary">
<h2>5.7 Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>Likelihood is a fundamental concept in statistical inference. In this lecture we have introduced the definition of likelihood and demonstrated how it can be used to estimate an unknown parameter, through maximum likelihood estimation. For two examples, we have seen that estimates obtained by MLE are intuitively sensible for the parameter of interest and have derived them algebraically via the log-likelihood.</p>
<p>In the next lecture, we will find out about the specific mathematical properties which make the MLE a “good” estimator, and extend to the situation where our data consist of more than one observation.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./05"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-versus-inference">5.1 Probability versus Inference</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimation">5.2 Maximum likelihood estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-likelihood">5.3 The likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-likelihoods">5.4 Examples of likelihoods¶</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binomial-model">5.4.1 Binomial model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exponential-model">5.4.2 Exponential model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#log-likelihood">5.5 Log likelihood</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-mle">5.6 Finding the MLE</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">5.6.1 Binomial model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">5.6.2 Exponential model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.7 Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>